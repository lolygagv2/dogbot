norm1 = normalization([127.5, 127.5, 127.5], [127.5, 127.5, 127.5])

context_switch_param(mode=enabled)
allocator_param(enable_partial_row_buffers=disabled)

resources_param(strategy=greedy, max_compute_utilization=0.8, max_control_utilization=0.95, max_memory_utilization=0.8)

model_optimization_config(calibration, batch_size=16, calibset_size=1024)
model_optimization_config(globals, multiproc_policy=disabled)
post_quantization_optimization(finetune, policy=enabled, batch_size=4)
pre_quantization_optimization(ew_add_fusing, policy=disabled)
model_optimization_flavor(optimization_level=0, compression_level=0)

pre_quantization_optimization(matmul_correction, layers={matmul*}, correction_type=zp_comp_block)
model_optimization_config(negative_exponent, layers={*}, rank=0)

quantization_param([vit_tiny/conv32], precision_mode=a16_w16)
quantization_param([vit_tiny/conv33], precision_mode=a16_w16)

quantization_param({vit_tiny/ew_add*}, precision_mode=a16_w16)
quantization_param({vit_tiny/ew_add1}, precision_mode=a8_w8)
