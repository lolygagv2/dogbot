base:
- base/coco_single_person.yaml
preprocessing:
  network_type: single_person_pose_estimation
  meta_arch: vit_pose
postprocessing:
  network_type: vit_pose
network:
  network_name: vit_pose_small
paths:
  alls_script: vit_pose_small.alls
  network_path:
  - models_files/SinglePersonPoseEstimation/vit/vit_pose_small/pretrained/2023-11-14/vit_pose_small.onnx
  url: https://hailo-model-zoo.s3.eu-west-2.amazonaws.com/SinglePersonPoseEstimation/vit/vit_pose_small/pretrained/2023-11-14/vit_pose_small.zip
parser:
  normalization_params:
    normalize_in_net: true
    mean_list:
    - 123.675
    - 116.28
    - 103.53
    std_list:
    - 58.395
    - 57.12
    - 57.375
evaluation:
  infer_type: np_infer
info:
  task: single person pose estimation
  input_shape: 256x192x3
  output_shape: 64x48x17
  operations: 17.17G
  parameters: 24.29M
  framework: pytorch
  eval_metric: AP
  full_precision_result: 74.16
  source: https://github.com/ViTAE-Transformer/ViTPose
  license_url: https://github.com/ViTAE-Transformer/ViTPose/blob/main/LICENSE
  license_name: Apache-2.0
