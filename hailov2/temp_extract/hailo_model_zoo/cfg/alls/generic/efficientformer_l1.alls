norm1 = normalization([123.675, 116.28, 103.53], [58.395, 57.12, 57.375])

model_optimization_config(globals, multiproc_policy=disabled)
pre_quantization_optimization(ew_add_fusing, policy=disabled)

post_quantization_optimization(finetune, policy=enabled, batch_size=4, epochs=12, def_loss_type=ce, learning_rate=0.0001)
model_optimization_flavor(optimization_level=0, compression_level=0)
pre_quantization_optimization(matmul_correction, layers={matmul*}, correction_type=zp_comp_block)
quantization_param(input_layer1, precision_mode=a16_w16)
quantization_param({ew_add*}, precision_mode=a16_w16)
quantization_param([conv1], precision_mode=a16_w16)
quantization_param([conv2], precision_mode=a16_w16)
quantization_param([conv3], precision_mode=a16_w16)
quantization_param([conv4], precision_mode=a16_w16)
quantization_param([conv5], precision_mode=a16_w16)
quantization_param([conv6], precision_mode=a16_w16)
quantization_param([conv7], precision_mode=a16_w16)
quantization_param([conv8], precision_mode=a16_w16)
quantization_param([conv9], precision_mode=a16_w16)
quantization_param([conv10], precision_mode=a16_w16)
quantization_param([conv11], precision_mode=a16_w16)
quantization_param([conv12], precision_mode=a16_w16)
quantization_param([conv13], precision_mode=a16_w16)
quantization_param([conv14], precision_mode=a16_w16)
quantization_param([conv15], precision_mode=a16_w16)
quantization_param([conv16], precision_mode=a16_w16)
quantization_param([conv17], precision_mode=a16_w16)
quantization_param([conv18], precision_mode=a16_w16)
quantization_param([conv19], precision_mode=a16_w16)
quantization_param([conv20], precision_mode=a16_w16)
quantization_param([conv21], precision_mode=a16_w16)
quantization_param([conv22], precision_mode=a16_w16)
quantization_param([conv22], precision_mode=a16_w16)
quantization_param([conv23], precision_mode=a16_w16)
quantization_param([conv24], precision_mode=a16_w16)
quantization_param([conv25], precision_mode=a16_w16)
quantization_param([conv26], precision_mode=a16_w16)
quantization_param([conv27], precision_mode=a16_w16)
quantization_param([conv28], precision_mode=a16_w16)
quantization_param([conv29], precision_mode=a16_w16)
quantization_param([conv30], precision_mode=a16_w16)
quantization_param([conv31], precision_mode=a16_w16)
quantization_param([conv32], precision_mode=a16_w16)
quantization_param([conv33], precision_mode=a16_w16)
quantization_param({mul_and_add*}, precision_mode=a16_w16)
quantization_param(efficientformer_l1/output_layer1,  precision_mode=a16_w16)
